{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto Generative - Redes Neurais","text":"<p>Este trabalho tem como objetivo explorar o funcionamento e as aplica\u00e7\u00f5es da Intelig\u00eancia Artificial Generativa, com foco em modelos de convers\u00e3o entre texto e imagem. A proposta consiste em construir e analisar pipelines de Text-to-Image e Image-to-Text utilizando a ferramenta Comfy UI, uma interface modular e visual que permite a cria\u00e7\u00e3o de fluxos complexos de infer\u00eancia em modelos de difus\u00e3o.</p> <p>A primeira parte do projeto investiga o processo de Text-to-Image, em que um modelo de difus\u00e3o \u00e9 guiado por descri\u00e7\u00f5es textuais para gerar imagens sint\u00e9ticas de alta fidelidade e coer\u00eancia sem\u00e2ntica. Foram configurados n\u00f3s de CLIP Text Encoder, Noise Sampler e Diffusion Model dentro do ComfyUI, permitindo compreender como a interpreta\u00e7\u00e3o textual influencia as etapas de ru\u00eddo, refinamento e renderiza\u00e7\u00e3o da imagem final. Essa fase enfatiza o papel da engenharia de prompts e dos par\u00e2metros de infer\u00eancia na qualidade e estilo das imagens geradas.</p> <p>Na segunda parte, o pipeline Image-to-Text inverte o processo: a partir de uma imagem de entrada, o sistema utiliza modelos de captioning e CLIP Vision Encoder para produzir descri\u00e7\u00f5es autom\u00e1ticas em linguagem natural, simulando uma forma de \u201ccompreens\u00e3o\u201d visual por parte da IA. Essa abordagem demonstra a integra\u00e7\u00e3o entre modelos de vis\u00e3o e linguagem, evidenciando como representa\u00e7\u00f5es latentes podem ser traduzidas entre dom\u00ednios multimodais.</p> <p>O uso do ComfyUI permitiu a experimenta\u00e7\u00e3o pr\u00e1tica e visual desses fluxos, favorecendo o entendimento da estrutura interna dos modelos generativos e suas rela\u00e7\u00f5es entre embeddings textuais, vetores latentes e processos de difus\u00e3o. O projeto, portanto, n\u00e3o apenas demonstra a capacidade criativa das IAs generativas, mas tamb\u00e9m aprofunda a compreens\u00e3o de seus fundamentos t\u00e9cnicos, refor\u00e7ando a import\u00e2ncia da modularidade, da parametriza\u00e7\u00e3o e da interpreta\u00e7\u00e3o cr\u00edtica dos resultados obtidos.</p>"},{"location":"#grupo","title":"Grupo","text":"<ol> <li>Esdras Gomes Carvalho</li> <li>Lincoln Rodrigo Pereira de Melo</li> <li>Lucca d'Oliveira Gheti Kao</li> </ol>"},{"location":"implementacao/text_audio/","title":"Texto para \u00c1udio","text":"<p>Esta implementa\u00e7\u00e3o utilizando o ComfyUI foi voltada para a transforma\u00e7\u00e3o de um prompt em texto em \u00e1udio/m\u00fasica. Essa t\u00e9cnica faz parte da classe de modelos conhecidos como Text-to-Audio Diffusion Models, que utilizam redes neurais para gerar \u00e1udio original a partir de descri\u00e7\u00f5es textuais. Nesta se\u00e7\u00e3o, ser\u00e1 explicado o workflow estruturado para essa etapa e o papel de cada elemento do pipeline. A figura a seguir mostra o fluxo completo dentro do ComfyUI, em que cada caixa representa um componente espec\u00edfico do processo.</p> <p></p> <p>Workflow completo de texto para \u00e1udio.</p> <p>Produ\u00e7\u00e3o autoral</p> <p>O pipeline se inicia com o n\u00f3 Load Checkpoint, respons\u00e1vel por carregar o modelo principal de difus\u00e3o \u2014 no caso, o modelo ace_step_v1_3.5b.safetensors. Esse \u00e9 o modelo AceStep, uma adapta\u00e7\u00e3o do AudioCraft para o ComfyUI, especializado em gerar m\u00fasica e voz a partir de texto. O checkpoint cont\u00e9m os pesos aprendidos da rede neural, que s\u00e3o o resultado de um treinamento pr\u00e9vio em milh\u00f5es de pares de \u00e1udio e descri\u00e7\u00f5es textuais. Assim, \u00e9 nele que est\u00e3o armazenados o conhecimento sonoro e a capacidade de associa\u00e7\u00e3o entre palavras e elementos musicais do modelo. O n\u00f3 fornece duas sa\u00eddas principais:</p> <ul> <li>MODEL, que cont\u00e9m a rede de difus\u00e3o propriamente dita (usada pelo KSampler);</li> <li>CLIP, que \u00e9 o m\u00f3dulo de processamento textual.</li> </ul> <p>A seguir, entra em cena o n\u00f3 TextEncodeAceStepAudio. Similar ao CLIP usado em modelos de imagem, este componente transforma o texto fornecido em uma representa\u00e7\u00e3o num\u00e9rica chamada vetor de embeddings, que descreve semanticamente o conte\u00fado do texto. No pipeline de \u00e1udio, o prompt pode conter:</p> <ol> <li>Descri\u00e7\u00f5es do estilo musical desejado (por exemplo: \"funk, pop, guitar, happy, lighthearted\");</li> <li>Letras completas da m\u00fasica a ser gerada.</li> </ol> <p>Esse vetor de condicionamento \u00e9 enviado ao pr\u00f3ximo n\u00f3, que \u00e9 o KSampler.</p> <p>Antes do KSampler, h\u00e1 tamb\u00e9m o n\u00f3 EmptyAceStepLatentAudio. Ele \u00e9 respons\u00e1vel por criar um \"\u00e1udio inicial\" no espa\u00e7o latente, um espa\u00e7o matem\u00e1tico de menor dimensionalidade onde o modelo trabalha durante a difus\u00e3o. Em vez de gerar o \u00e1udio diretamente em forma de onda, o modelo opera nesse espa\u00e7o comprimido, o que reduz o custo computacional e permite maior estabilidade. Nesse n\u00f3, define-se:</p> <ul> <li>seconds: a dura\u00e7\u00e3o do \u00e1udio a ser gerado (por exemplo, 30.0 segundos);</li> <li>batch_size: o n\u00famero de faixas que ser\u00e3o geradas simultaneamente.</li> </ul> <p>Como o nome indica, esse \u00e1udio latente \u00e9 inicialmente \"vazio\", preenchido com ru\u00eddo aleat\u00f3rio.</p> <p>Entre o espa\u00e7o latente e o KSampler, h\u00e1 o n\u00f3 LatentOperationTonemapReinhard, que ajusta o volume ou intensidade do som dentro do espa\u00e7o latente. O par\u00e2metro multiplier controla o ganho, permitindo aumentar ou diminuir o volume global da sa\u00edda vocal antes mesmo da decodifica\u00e7\u00e3o final.</p> <p>O KSampler \u00e9 o n\u00facleo do processo de gera\u00e7\u00e3o. Ele \u00e9 o componente respons\u00e1vel por \"guiar\" o ru\u00eddo inicial para uma forma coerente, de acordo com o conte\u00fado textual definido pelo prompt. Esse processo \u00e9 conhecido como difus\u00e3o reversa, onde o modelo aprende a transformar gradualmente o ru\u00eddo em \u00e1udio, passo a passo.</p> <p>Dentro desse n\u00f3, cada par\u00e2metro controla um aspecto essencial do comportamento do modelo:</p> <ul> <li>model: conecta o modelo principal carregado no checkpoint.</li> <li>positive: \u00e9 o condicionamento de texto, oriundo do TextEncodeAceStepAudio.</li> <li>latent_image: \u00e9 a entrada de ru\u00eddo gerada pelo EmptyAceStepLatentAudio.</li> <li>seed: define a aleatoriedade da gera\u00e7\u00e3o; mudar o seed altera os detalhes sonoros, mesmo com o mesmo prompt.</li> <li>steps: n\u00famero de etapas de amostragem. Quanto mais etapas, mais o \u00e1udio ser\u00e1 detalhado, por\u00e9m ser\u00e1 mais lento para gerar.</li> <li>cfg (Classifier-Free Guidance): controla o quanto o modelo deve \"seguir\" o texto \u2014 valores altos aumentam a fidelidade ao prompt, mas podem introduzir artefatos.</li> <li>sampler_name: define o algoritmo usado para a difus\u00e3o, cada um com seu estilo de converg\u00eancia (por exemplo, euler).</li> <li>scheduler: regula a progress\u00e3o da remo\u00e7\u00e3o de ru\u00eddo ao longo dos passos, influenciando suavidade e coer\u00eancia.</li> <li>denoise: define a intensidade da difus\u00e3o \u2014 valores menores permitem refinar ou preservar partes do \u00e1udio, \u00fateis para edi\u00e7\u00f5es.</li> </ul> <p>O resultado final desse n\u00f3 \u00e9 um \u00e1udio latente j\u00e1 estruturado, mas ainda n\u00e3o em formato de onda sonora compreens\u00edvel.</p> <p>Por fim, esse \u00e1udio latente \u00e9 enviado ao VAEDecodeAudio, cuja fun\u00e7\u00e3o \u00e9 decodificar o espa\u00e7o latente em um \u00e1udio real, em forma de onda (waveform). Similar ao VAE usado em modelos de imagem, este componente \u00e9 respons\u00e1vel por converter o espa\u00e7o latente em um sinal de \u00e1udio que pode ser reproduzido. A qualidade e fidelidade da reconstru\u00e7\u00e3o dependem do decodificador utilizado.</p> <p>O resultado do VAE \u00e9, ent\u00e3o, enviado ao n\u00f3 Save Audio (MP3), que exporta o \u00e1udio final para um arquivo MP3 e permite a pr\u00e9-visualiza\u00e7\u00e3o do resultado. Nesse ponto, a convers\u00e3o de texto em \u00e1udio est\u00e1 completa: a descri\u00e7\u00e3o textual fornecida foi traduzida, pelo processo de difus\u00e3o e decodifica\u00e7\u00e3o, em uma faixa de \u00e1udio coerente.</p> <p>Em resumo, o workflow segue a seguinte l\u00f3gica conceitual:</p> <ol> <li>Carregar o modelo base (Load Checkpoint)</li> <li>Codificar o texto (TextEncodeAceStepAudio)</li> <li>Gerar um \u00e1udio latente inicial (EmptyAceStepLatentAudio)</li> <li>Ajustar o volume no espa\u00e7o latente (LatentOperationTonemapReinhard)</li> <li>Aplicar a difus\u00e3o guiada pelo prompt (KSampler)</li> <li>Decodificar o espa\u00e7o latente em forma de onda (VAEDecodeAudio)</li> <li>Salvar e visualizar o resultado (Save Audio)</li> </ol> <p>Esse pipeline ilustra o funcionamento fundamental dos modelos generativos de \u00e1udio: transformar linguagem natural em representa\u00e7\u00f5es sonoras por meio de redes neurais que compreendem tanto o texto quanto a estrutura musical e vocal. O processo como um todo \u00e9 baseado na difus\u00e3o do \u00e1udio, seguindo princ\u00edpios similares aos modelos de imagem.</p> <p>A difus\u00e3o no contexto de \u00e1udio funciona de maneira an\u00e1loga \u00e0 difus\u00e3o de imagens: come\u00e7amos de \"ru\u00eddo puro\" (ru\u00eddo gaussiano no espa\u00e7o latente) e, passo a passo, vamos removendo o ru\u00eddo certo at\u00e9 que reste um \u00e1udio coerente. Durante o treinamento, o modelo aprende duas opera\u00e7\u00f5es complementares:</p> <ol> <li>a difus\u00e3o direta (forward diffusion), que pega um \u00e1udio real e adiciona ru\u00eddo gradualmente at\u00e9 ficar irreconhec\u00edvel;</li> <li>a difus\u00e3o reversa (reverse diffusion), que aprende a prever o ru\u00eddo presente num estado intermedi\u00e1rio e, ao remov\u00ea-lo, aproximar esse estado de um \u00e1udio plaus\u00edvel.</li> </ol> <p>Na gera\u00e7\u00e3o, s\u00f3 usamos a parte reversa, partindo do ru\u00eddo e repetindo o processo de \"prever ru\u00eddo \u2192 remover ru\u00eddo\" por um certo n\u00famero de steps at\u00e9 chegar a um latente limpo. No workflow em quest\u00e3o, quem executa essas itera\u00e7\u00f5es \u00e9 o KSampler. A cada passo, o modelo recebe: o latente ruidoso, o timestep (que diz \"quanto ru\u00eddo ainda deve existir\") e o condicionamento de texto (os vetores produzidos pelo TextEncodeAceStepAudio). Ele devolve a previs\u00e3o de ru\u00eddo, que o sampler usa para atualizar o latente. O scheduler controla o \"ritmo\" dessa limpeza (a curva de ru\u00eddo ao longo do tempo), enquanto o sampler define o m\u00e9todo num\u00e9rico de integra\u00e7\u00e3o. Mais steps tendem a refinar mais, por\u00e9m com retornos decrescentes e maior custo.</p> <p>O condicionamento textual atua via Classifier-Free Guidance (CFG). Em termos simples, o modelo faz duas previs\u00f5es de ru\u00eddo: uma sem texto (previs\u00e3o unconditional) e outra com texto (previs\u00e3o conditional). Elas s\u00e3o combinadas por um fator de guia (o par\u00e2metro cfg):</p> \\[ \\hat{\\epsilon} = \\hat{\\epsilon}_{\\text{uncond}} + s \\cdot (\\hat{\\epsilon}_{\\text{cond}} - \\hat{\\epsilon}_{\\text{uncond}}) \\] <p>Quando o valor de CFG \u00e9 baixo, o \u00e1udio pode ficar natural, por\u00e9m pouco fiel ao prompt. Quando \u00e9 alto, segue fielmente o texto, mas pode introduzir artefatos sonoros ou distor\u00e7\u00f5es.</p> <p>Por fim, o par\u00e2metro denoise define quanto desse processo de limpeza ser\u00e1 aplicado. Com denoise = 1.0, voc\u00ea realiza a cadeia completa (\u00fatil para text-to-audio puro). Com valores menores, preserva-se parte da estrutura do latente de entrada \u2014 isso \u00e9 a base de refinamentos e edi\u00e7\u00f5es de \u00e1udio.</p> <p>Em suma, o modelo come\u00e7a no ru\u00eddo (EmptyAceStepLatentAudio), o KSampler executa m\u00faltiplos passos de remo\u00e7\u00e3o de ru\u00eddo guiada pelo texto (via TextEncodeAceStepAudio e CFG), o scheduler/sampler dita o caminho num\u00e9rico dessa caminhada, e o VAEDecodeAudio decodifica o latente final para forma de onda que voc\u00ea pode ouvir. Essa dan\u00e7a entre ru\u00eddo, guia textual e integra\u00e7\u00e3o num\u00e9rica \u00e9 o que transforma um prompt em uma faixa de \u00e1udio consistente.</p>"},{"location":"implementacao/text_image/","title":"Texto para Imagem","text":"<p>Como dito anteriormente, essa primeira implementa\u00e7\u00e3o utilizando o ComfyUI foi voltada para a transforma\u00e7\u00e3o de um prompt em texto em uma imagem. Essa t\u00e9cnica faz parte da classe de modelos conhecidos como Text-to-Image Diffusion Models, que utilizam redes neurais para gerar imagens originais a partir de descri\u00e7\u00f5es textuais. Nesta se\u00e7\u00e3o, ser\u00e1 explicado o workflow estruturado para essa etapa e o papel de cada elemento do pipeline. A figura a seguir mostra o fluxo completo dentro do ComfyUI, em que cada caixa representa um componente espec\u00edfico do processo.</p> <p></p> <p>Workflow completo de texto para imagem.</p> <p>Produ\u00e7\u00e3o autoral</p> <p>O pipeline se inicia com o n\u00f3 Load Checkpoint, respons\u00e1vel por carregar o modelo principal de difus\u00e3o \u2014 no caso, o modelo SD15_cartoon.safetensors. Esse nome indica uma varia\u00e7\u00e3o do Stable Diffusion 1.5, mas treinada para gerar imagens em um estilo cartoon. O checkpoint cont\u00e9m os pesos aprendidos da rede neural, que s\u00e3o o resultado de um treinamento pr\u00e9vio em milh\u00f5es de pares de imagens e descri\u00e7\u00f5es textuais. Assim, \u00e9 nele que est\u00e3o armazenados o conhecimento visual e a capacidade de associa\u00e7\u00e3o entre palavras e elementos visuais do modelo. Esse modelo j\u00e1 treinado foi baixado do site CIVITAI, sendo necess\u00e1rio apenas a instala\u00e7\u00e3o e aplica\u00e7\u00e3o no projeto. Importante citar aqui que esse modelo baixado tem o estilo de cartoon, por\u00e9m na fonte utilizada, \u00e9 poss\u00edvel encontrar diversos outros modelos do mesmo tipo (Stable Diffusion 1.5) mas com outro foco de treinamento que vai entregar outros valores para os par\u00e2metros, e portanto, outro estilo para a imagem. O n\u00f3 tamb\u00e9m fornece as tr\u00eas sa\u00eddas principais do modelo:</p> <ul> <li>MODEL, que cont\u00e9m a rede de difus\u00e3o propriamente dita (usada pelo KSampler);</li> <li>CLIP, que \u00e9 o m\u00f3dulo de processamento textual;</li> <li>VAE, que \u00e9 o m\u00f3dulo de codifica\u00e7\u00e3o e decodifica\u00e7\u00e3o das imagens em um espa\u00e7o latente comprimido.</li> </ul> <p>A seguir, entram em cena dois n\u00f3s chamados CLIP Text Encode (Prompt). O CLIP \u2014 Contrastive Language\u2013Image Pre-training \u2014 \u00e9 uma rede neural desenvolvida para compreender a rela\u00e7\u00e3o entre texto e imagem. Ele transforma o texto fornecido em uma representa\u00e7\u00e3o num\u00e9rica chamada vetor de embeddings, que descreve semanticamente o conte\u00fado do texto. No pipeline, h\u00e1 dois CLIPs:</p> <ol> <li>O primeiro \u00e9 o prompt positivo, que cont\u00e9m a descri\u00e7\u00e3o desejada da imagem:</li> <li>O segundo \u00e9 o prompt negativo, que especifica o que n\u00e3o deve aparecer na imagem. Essa etapa \u00e9 essencial para orientar o modelo a evitar erros comuns e melhorar a qualidade final da sa\u00edda.</li> </ol> <p>Esses dois vetores de condicionamento s\u00e3o enviados ao pr\u00f3ximo n\u00f3, que \u00e9 o KSampler.</p> <p>Antes do KSampler, h\u00e1 tamb\u00e9m o n\u00f3 Empty Latent Image. Ele \u00e9 respons\u00e1vel por criar uma \u201cimagem inicial\u201d no espa\u00e7o latente, um espa\u00e7o matem\u00e1tico de menor dimensionalidade onde o modelo trabalha durante a difus\u00e3o. Em vez de gerar a imagem diretamente em pixels, o Stable Diffusion opera nesse espa\u00e7o comprimido, o que reduz o custo computacional e permite maior estabilidade. Nesse n\u00f3, define-se o tamanho da imagem e o n\u00famero de imagens que ser\u00e3o geradas (batch size). Como o nome indica, essa imagem \u00e9 inicialmente \u201cvazia\u201d, preenchida com ru\u00eddo aleat\u00f3rio.</p> <p>O KSampler \u00e9 o n\u00facleo do processo de gera\u00e7\u00e3o. Ele \u00e9 o componente respons\u00e1vel por \u201cguiar\u201d o ru\u00eddo inicial para uma forma coerente, de acordo com o conte\u00fado textual definido pelos prompts. Esse processo \u00e9 conhecido como difus\u00e3o reversa, onde o modelo aprende a transformar gradualmente o ru\u00eddo em imagem, passo a passo. Dentro desse n\u00f3, cada par\u00e2metro controla um aspecto essencial do comportamento do modelo:</p> <ul> <li>model: conecta o modelo principal carregado no checkpoint.</li> <li>positive e negative: s\u00e3o os condicionamentos de texto, oriundos dos CLIP encoders.</li> <li>latent_image: \u00e9 a entrada de ru\u00eddo gerada pelo Empty Latent Image.</li> <li>seed: define a aleatoriedade da gera\u00e7\u00e3o; mudar o seed altera os detalhes visuais, mesmo com o mesmo prompt.</li> <li>steps: n\u00famero de etapas de amostragem. Quanto mais etapas, mais a imagem vai ser detalhada, por\u00e9m ser\u00e1 mais lenta para gerar.</li> <li>cfg (Classifier-Free Guidance): controla o quanto o modelo deve \u201cseguir\u201d o texto \u2014 valores altos aumentam a fidelidade ao prompt, mas podem reduzir a naturalidade da imagem.</li> <li>sampler_name: define o algoritmo usado para a difus\u00e3o, cada um com seu estilo de converg\u00eancia.</li> <li>scheduler: regula a progress\u00e3o da remo\u00e7\u00e3o de ru\u00eddo ao longo dos passos, influenciando suavidade e coer\u00eancia.</li> <li>denoise: define a intensidade da difus\u00e3o \u2014 valores menores permitem refinar ou preservar partes da imagem, \u00fateis para edi\u00e7\u00f5es.</li> </ul> <p>O resultado final desse n\u00f3 \u00e9 uma imagem latente j\u00e1 estruturada, mas ainda n\u00e3o em formato visual compreens\u00edvel.</p> <p>Por fim, essa imagem latente \u00e9 enviada ao VAE Decode, cuja fun\u00e7\u00e3o \u00e9 decodificar o espa\u00e7o latente em uma imagem real, em pixels. O termo VAE vem de Variational Autoencoder, um tipo de rede neural usada para compress\u00e3o e reconstru\u00e7\u00e3o de dados. No contexto do Stable Diffusion, o VAE \u00e9 respons\u00e1vel por converter o espa\u00e7o latente em uma imagem RGB que pode ser visualizada. A qualidade e fidelidade da reconstru\u00e7\u00e3o dependem muito do VAE utilizado, e por isso \u00e9 comum usar VAEs espec\u00edficos \u2014 no caso representado o VAE utilizado foi o carregado junto ao SD15_cartoon.</p> <p>O resultado do VAE \u00e9, ent\u00e3o, enviado ao n\u00f3 Preview Image, que exibe a imagem final na interface do ComfyUI. Nesse ponto, a convers\u00e3o de texto em imagem est\u00e1 completa: a descri\u00e7\u00e3o textual fornecida ao CLIP foi traduzida, pelo processo de difus\u00e3o e decodifica\u00e7\u00e3o, em uma imagem visual coerente, gerando o pinguim surfando em um estilo cartoon mostrado na pr\u00e9-visualiza\u00e7\u00e3o.</p> <p>Em resumo, o workflow segue a seguinte l\u00f3gica conceitual:</p> <ol> <li>Carregar o modelo base (Load Checkpoint)</li> <li>Codificar os textos positivos e negativos (CLIP Text Encode)</li> <li>Gerar uma imagem latente inicial (Empty Latent Image)</li> <li>Aplicar a difus\u00e3o guiada pelos prompts (KSampler)</li> <li>Decodificar o espa\u00e7o latente em pixels (VAE Decode)</li> <li>Visualizar o resultado (Preview Image)</li> </ol> <p>Esse pipeline ilustra o funcionamento fundamental dos modelos generativos modernos: transformar linguagem natural em representa\u00e7\u00f5es visuais por meio de redes neurais que compreendem tanto o texto quanto a estrutura visual do mundo. O processo como um todo \u00e9 baseado na difus\u00e3o da imagem. Uma boa maneira de imaginar a difus\u00e3o \u00e9 pensar numa foto coberta de poeira: a imagem existe, mas est\u00e1 escondida sob camadas de ru\u00eddo. O modelo aprende a tirar a poeira, aos poucos, at\u00e9 revelar um conte\u00fado coerente. Nos modelos modernos, fazemos o inverso de como foram treinados: come\u00e7amos de \u201cpoeira pura\u201d (ru\u00eddo gaussiano no espa\u00e7o latente) e, passo a passo, vamos removendo o ru\u00eddo certo at\u00e9 que reste uma imagem.</p> <p>Durante o treinamento, o modelo aprende duas opera\u00e7\u00f5es complementares:</p> <ol> <li>a difus\u00e3o direta (forward diffusion), que pega uma imagem real e adiciona ru\u00eddo gradualmente at\u00e9 ficar irreconhec\u00edvel;</li> <li>a difus\u00e3o reversa (reverse diffusion), que aprende a prever o ru\u00eddo presente num estado intermedi\u00e1rio e, ao remov\u00ea-lo, aproximar esse estado de uma imagem plaus\u00edvel.</li> </ol> <p>Na gera\u00e7\u00e3o, s\u00f3 usamos a parte reversa, partindo do ru\u00eddo e repetindo o processo de \u201cprever ru\u00eddo \u2192 remover ru\u00eddo\u201d por um certo n\u00famero de steps at\u00e9 chegar a um latente limpo. No workflow em quest\u00e3o, quem executa essas itera\u00e7\u00f5es \u00e9 o KSampler. A cada passo (t), o modelo (um U-Net dentro do Stable Diffusion) recebe: o latente ruidoso, o timestep (que diz \u201cquanto ru\u00eddo ainda deve existir\u201d) e o condicionamento de texto (os vetores produzidos pelos dois CLIPs). Ele devolve a previs\u00e3o de ru\u00eddo (\\hat{\\epsilon}), que o sampler usa para atualizar o latente. O scheduler controla o \u201critmo\u201d dessa limpeza (a curva de ru\u00eddo ao longo do tempo), enquanto o sampler define o m\u00e9todo num\u00e9rico de integra\u00e7\u00e3o (LMS, Euler, DPM++ etc.). Mais steps tendem a refinar mais, por\u00e9m com retornos decrescentes e maior custo.</p> <p>O condicionamento textual atua via Classifier-Free Guidance (CFG). Em termos simples, o modelo faz duas previs\u00f5es de ru\u00eddo: uma sem texto (previs\u00e3o unconditional) e outra com texto (previs\u00e3o conditional). Elas s\u00e3o combinadas por um fator de guia (s) (o seu cfg):</p> \\[\\hat{\\epsilon} = \\hat{\\epsilon}*{\\text{unconditional}} + s,\\big(\\hat{\\epsilon}*{\\text{conditional}} - \\hat{\\epsilon}_{\\text{unconditional}}\\big)\\] <p>Quando (s) \u00e9 baixo, a imagem pode ficar bonita, por\u00e9m pouco fiel ao prompt. Quando \u00e9 alto, segue fielmente o texto, mas pode introduzir satura\u00e7\u00e3o, artificiais ou banding. Os prompts negativo e positivo entram exatamente aqui, puxando a previs\u00e3o condicional para aproximar o que voc\u00ea descreveu e \u201cafastar\u201d o que voc\u00ea rejeitou.</p> <p>Por fim, o par\u00e2metro denoise define quanto desse processo de limpeza ser\u00e1 aplicado. Com denoise = 1.0, voc\u00ea realiza a cadeia completa (\u00fatil para text-to-image puro). Com valores menores, preserva-se parte da estrutura do latente de entrada \u2014 isso \u00e9 a base do image-to-image e de refinamentos: o sampler aplica apenas um \u201ctrecho\u201d da caminhada de ru\u00eddo \u2192 imagem, mantendo composi\u00e7\u00e3o ou cores originais.</p> <p>Em suma, o modelo come\u00e7a no ru\u00eddo (Empty Latent Image), o KSampler executa m\u00faltiplos passos de remo\u00e7\u00e3o de ru\u00eddo guiada pelo texto (via CLIP e CFG), o scheduler/sampler dita o caminho num\u00e9rico dessa caminhada, e o VAE decodifica o latente final para pixels que voc\u00ea v\u00ea no Preview. Essa dan\u00e7a entre ru\u00eddo, guia textual e integra\u00e7\u00e3o num\u00e9rica \u00e9 o que transforma um prompt em uma imagem consistente.</p>"},{"location":"resultados/text_to_audio/","title":"Resultados - Texto para \u00c1udio","text":"<p>Nesta sec\u00e7\u00e3o s\u00e3o apresentados os \u00e1udios gerados com o workflow de Texto para \u00c1udio (ComfyUI), seguindo o esp\u00edrito do relat\u00f3rio de imagens. Para cada output, est\u00e3o o guia (prompt), o player, a letra e coment\u00e1rios breves.</p>"},{"location":"resultados/text_to_audio/#1o-output-pagodinho-100110-bpm","title":"1\u00ba OUTPUT \u2014 (Pagodinho, 100\u2013110 BPM)","text":""},{"location":"resultados/text_to_audio/#audio","title":"\u00c1udio","text":"<p>    Your browser does not support the audio element. </p>"},{"location":"resultados/text_to_audio/#letra","title":"Letra","text":"<p>(Verso 1) No corredor da sala vem chegando devagar, Baixinho no passo, mas gigante no pensar. Bacharel e doutor, Poli e Max-Planck no altar, Tecendo redes no sil\u00eancio, fazendo o mundo calcular.</p> <p>(Pr\u00e9-refr\u00e3o) Com G\u00f6del na mochila e perguntas no olhar, Procura na biologia um jeito de ensinar.</p> <p>(Refr\u00e3o) Ele \u00e9 o professor das redes neurais, Mistura ci\u00eancia, poesia e sinais. No quadro risca rumo a novos cais, E a gente aprende a ver o mundo em m\u00faltiplos sinais. Oh, professor \u2014 nosso mapa e farol \u2014 Pequeno na altura, imenso no rol.</p> <p>(Verso 2) Fazendo do caos padr\u00e3o, do tempo, previs\u00e3o, Neurofuzzy, spikes, din\u00e2mica e intui\u00e7\u00e3o. Criptografia, bancos, alto desempenho na m\u00e3o, Empreende, ensina, traduz complexa can\u00e7\u00e3o.</p> <p>(Pr\u00e9-refr\u00e3o) L\u00ea filosofia no caf\u00e9, discute pol\u00edtica no \u00f4nibus, Mistura teoria e pr\u00e1tica sem medo do \u00f3bvio.</p> <p>(Refr\u00e3o) Ele \u00e9 o professor das redes neurais, Mistura ci\u00eancia, poesia e sinais. No quadro risca rumo a novos cais, E a gente aprende a ver o mundo em m\u00faltiplos sinais. Oh, professor \u2014 nosso mapa e farol \u2014 Pequeno na altura, imenso no rol.</p> <p>(Ponte) Das confer\u00eancias ao laborat\u00f3rio ele vai, Com artigos, c\u00f3digos e um jeitinho sagaz. Mostra que a mente \u00e9 rede que aprende a rimar, E que os maiores problemas podem dialogar.</p> <p>(Verso 3) Do reconhecimento facial ao RFID no ch\u00e3o, Projetos que inventam um jeito novo de solu\u00e7\u00e3o. No banco, na aula, no start-up a brotar, Ele planta perguntas pra gente cultivar.</p> <p>(Refr\u00e3o \u2014 varia\u00e7\u00e3o) \u00c9 o senhor das redes, das \u00f3rbitas de spike, Ensina que pensar \u00e9 mais que um simples like. Baixinho no gesto, profundo no speech, Convida a curiosidade pra nunca ficar de beach.</p> <p>(Final / Outro) Quando a noite chega e a sala se apaga, Ficamos com o eco da aula que nos embala. Professor, obrigado por mostrar o portal \u2014 De que o saber \u00e9 rede, \u00e9 vida, \u00e9 sinal.</p> <p>\u2014 Para o nosso baixinho gigante das redes neurais.</p>"},{"location":"resultados/text_to_audio/#parametros-e-guia-prompt","title":"Par\u00e2metros e guia (prompt)","text":"<p>Create a lively Brazilian pagodinho track (100\u2013110 BPM) using the Portuguese lyrics provided. Acoustic arrangement with cavaquinho, pandeiro, tant\u00e3, surdo, acoustic guitar, upright bass and light brass or flute accents. Warm male lead vocal, intimate and cheerful; tight three-part backing harmonies on choruses. Bright, live-feel production with natural room ambience and punchy percussion.</p>"},{"location":"resultados/text_to_audio/#comentarios","title":"Coment\u00e1rios","text":"<p>Clima ao vivo e percuss\u00e3o marcada combinam com o tom celebrat\u00f3rio da letra. A escolha de backing harmonies nos refr\u00f5es refor\u00e7a o car\u00e1ter \u201chomenagem\u201d.</p>"},{"location":"resultados/text_to_audio/#2o-output-rocknu-metal-90100-bpm","title":"2\u00ba OUTPUT \u2014 (Rock/Nu-metal, 90\u2013100 BPM)","text":""},{"location":"resultados/text_to_audio/#audio_1","title":"\u00c1udio","text":"<p>    Your browser does not support the audio element. </p>"},{"location":"resultados/text_to_audio/#letra_1","title":"Letra","text":"<p>mesma letra do anterior</p>"},{"location":"resultados/text_to_audio/#parametros-e-guia-prompt_1","title":"Par\u00e2metros e guia (prompt)","text":"<p>Create a modern rock/nu-metal track inspired by Linkin Park, around 90\u2013100 BPM, using the Portuguese lyrics provided. Blend heavy electric guitars with distorted riffs, punchy bass, dynamic drums, and electronic textures (synth pads and glitchy samples). Alternate between melodic vocals and light rap-style verses, with emotional intensity and a powerful chorus. The lead vocal should sound passionate and slightly raspy, conveying admiration and curiosity \u2014 as if singing about a brilliant teacher who explores the mind through neural networks. Use cinematic transitions, a short instrumental bridge with a guitar solo or electronic break, and end with a dramatic chorus. Production should sound energetic, modern, and emotional, similar to Linkin Park\u2019s style from the \u2018Hybrid Theory\u2019 and \u2018Meteora\u2019 eras.</p>"},{"location":"resultados/text_to_audio/#comentarios_1","title":"Coment\u00e1rios","text":"<p>Guitarras com distor\u00e7\u00e3o e din\u00e2mica de versos/chorus d\u00e3o intensidade emocional adequada ao tom de admira\u00e7\u00e3o.</p>"},{"location":"resultados/text_to_audio/#3o-output-rocknu-metal-90100-bpm-versao-em-ingles","title":"3\u00ba OUTPUT \u2014 (Rock/Nu-metal, 90\u2013100 BPM \u2013 vers\u00e3o em ingl\u00eas)","text":""},{"location":"resultados/text_to_audio/#audio_2","title":"\u00c1udio","text":"<p>    Your browser does not support the audio element. </p>"},{"location":"resultados/text_to_audio/#letra_2","title":"Letra","text":"<p>(Verse 1) He walks the halls, quiet but bright, A spark of logic in the fading light. From S\u00e3o Paulo to Max Planck skies, He\u2019s mapping thoughts the brain implies.</p> <p>(Pre-Chorus) Equations burning in his eyes, He teaches us where chaos lies.</p> <p>(Chorus) Neural dreams, electric mind, He connects the dots we couldn\u2019t find. Through patterns deep and signals wild, He shows the code behind the smile. He\u2019s small in height but vast inside \u2014 A giant soul the code can\u2019t hide.</p> <p>(Verse 2) From fuzzy logic to the stream of time, He turns equations into rhyme. A master coder, teacher, friend, Who sees where logic has no end.</p> <p>(Pre-Chorus) He speaks of G\u00f6del, worlds so small, Yet his ideas reach through it all.</p> <p>(Chorus) Neural dreams, electric mind, He connects the dots we couldn\u2019t find. Through patterns deep and signals wild, He shows the code behind the smile. He\u2019s small in height but vast inside \u2014 A giant soul the code can\u2019t hide.</p> <p>(Bridge) Circuits pulse, the neurons sing, A symphony of everything. He says, \u201cTo think is more than know,\u201d And lights our path with data\u2019s glow.</p> <p>(Final Chorus) Neural dreams, electric mind, He breaks the walls that hold mankind. From chaos, he creates the flame, To teach us thought is more than game. He\u2019s small in height but vast inside \u2014 Humberto, the guide where reason hides.</p> <p>(Outro \u2014 whispered) \u201cBetween the spikes and the silence\u2026 we learn to think.\u201d</p>"},{"location":"resultados/text_to_audio/#4o-output-opera-sinfonica-3-atos","title":"4\u00ba OUTPUT \u2014 (\u00d3pera sinf\u00f4nica \u2014 3 atos)","text":""},{"location":"resultados/text_to_audio/#audio_3","title":"\u00c1udio","text":"<p>    Your browser does not support the audio element. </p>"},{"location":"resultados/text_to_audio/#letra_3","title":"Letra","text":"<p>Act I \u2014 The Spark of Thought</p> <p>(Grand orchestral overture, soft tenor solo) Tenor (Student): From silicon and soul he came, A thinker bound by none. In circuits deep he saw a flame, The light of reason\u2019s sun.</p> <p>Chorus (Students): Oh Humberto, voice of code and dream, Through neurons flows your stream. Your mind reflects a boundless scheme, Half logic\u2026 half supreme.</p> <p>Act II \u2014 G\u00f6del\u2019s Shadow</p> <p>(Dramatic strings and organ, baritone solo with chorus) Baritone (Humberto): I sought the truth through tangled signs, In patterns lost and found. Yet G\u00f6del whispers, \u201cThere are lines No theorem can surround.\u201d</p> <p>Chorus (Echoes of Thought): Yet he dares! He dares to go beyond! Through chaos he finds form! In neurons\u2019 dance, in data\u2019s song, He sees the human storm!</p> <p>(Orchestral crescendo \u2014 brass and timpani swell)</p> <p>Act III \u2014 The Symphony of the Mind</p> <p>(Hopeful finale \u2014 strings and choir rise together) Tenor (Student): Now the code becomes alive, Each pattern breathes anew. Through his wisdom we survive, And glimpse what minds can do.</p> <p>Full Chorus: Hail Humberto! Master of the spark! Your mind ignites the dark! In circuits, dreams, and reason\u2019s art, You built the world\u2019s new heart!</p>"},{"location":"resultados/text_to_audio/#parametros-e-guia-prompt_2","title":"Par\u00e2metros e guia (prompt)","text":"<p>Create a grand symphonic opera piece titled The Mind of Humberto, inspired by classical composers like Verdi, Puccini, and Philip Glass, with modern cinematic orchestration. Use the English lyrics provided.</p> <p>Structure: three acts \u2014 Act I (inspirational and curious), Act II (dark and dramatic, exploring philosophy and G\u00f6del\u2019s theorem), and Act III (uplifting and triumphant finale).</p> <p>Orchestration: full symphony (strings, brass, choir, timpani, harp, organ). Include tenor and baritone soloists, plus a mixed choir.</p> <p>Emotion: reverent, intellectual, and epic \u2014 a tribute to a brilliant professor who explores the mysteries of human computation and artificial intelligence.</p> <p>Tempo and tone: Begin lento (80 BPM, reflective), rise to allegro maestoso (100\u2013110 BPM) in the finale. Recording should sound like a live opera hall performance with rich reverb and dynamic vocals.</p>"},{"location":"resultados/text_to_audio/#5o-output-latin-popreggaeton-9095-bpm","title":"5\u00ba OUTPUT \u2014 (Latin pop/Reggaeton, 90\u201395 BPM)","text":""},{"location":"resultados/text_to_audio/#audio_4","title":"\u00c1udio","text":"<p>    Your browser does not support the audio element. </p>"},{"location":"resultados/text_to_audio/#letra_4","title":"Letra","text":"<p>[Intro \u2013 Voz suave, ritmo reggaet\u00f3n lento] (Hey) \u00c9l no es tan alto, pero piensa infinito, con su mente vuela por todo el circuito. Del Max Planck trajo el fuego bendito, \u00a1ay, profesor Humbertito!</p> <p>[Verso 1] Tiene el c\u00f3digo en el coraz\u00f3n, habla de spikes con tanta pasi\u00f3n. Cuando explica, hay revoluci\u00f3n, y la red aprende la lecci\u00f3n.</p> <p>[Pre-coro] Con un caf\u00e9, filosof\u00eda, y un toque de iron\u00eda, ense\u00f1a inteligencia artificial como una melod\u00eda.</p> <p>[Coro] \u00a1Humbertito! Tu mente brilla m\u00e1s que un sat\u00e9lite bonito. Tus neuronas bailan al comp\u00e1s del ritmo, bioinspirado, tan humano y tan distinto. Oh, profesor Humbertito...</p> <p>[Verso 2] Entre ecuaciones y emoci\u00f3n, construye sue\u00f1os en su lecci\u00f3n. Del caos saca una canci\u00f3n, la ciencia tiene su coraz\u00f3n.</p> <p>[Pre-coro 2] Cuando dice \u201cG\u00f6del lo sab\u00eda\u201d, nadie se atreve a discutir su poes\u00eda.</p> <p>[Coro 2] \u00a1Humbertito! El maestro que del c\u00f3digo hizo mito. Entre cables encontr\u00f3 lo infinito, y en su clase todos gritan: \u00a1repito!</p> <p>[Puente \u2013 rap suave tipo Daddy Yankee] (Yeah) Del Brasil hasta Alemania, rompiendo todas las fronteras, ense\u00f1ando que la mente humana es la red verdadera. No hay algoritmo que lo imite, ni IA que lo limite, cuando explica\u2026 el tiempo se derrite.</p> <p>[Coro final] \u00a1Humbertito! Tu l\u00f3gica nos gu\u00eda despacito, entre redes, caos y circuitos, nos ense\u00f1as que pensar es bonito. Ay, profesor Humbertito\u2026 \u00a1infinito!</p>"},{"location":"resultados/text_to_audio/#parametros-e-guia-prompt_3","title":"Par\u00e2metros e guia (prompt)","text":"<p>Create a Latin pop/reggaeton track in the style of Despacito (Luis Fonsi feat. Daddy Yankee) using the Spanish lyrics provided.</p> <p>Tempo: around 90\u201395 BPM.</p> <p>Mood: romantic, fun, and celebratory, but respectful \u2014 a playful homage to a brilliant professor.</p> <p>Instruments: acoustic guitar with flamenco flavor, reggaeton percussion (kick, snare, claps), subtle synth pads, and Latin bass groove.</p> <p>Vocals: warm male Latin pop lead, slightly flirtatious tone, with light rap/reggaeton backing in the bridge. Harmonized chorus vocals on each '\u00a1Humbertito!' line.</p> <p>Overall style: catchy, rhythmic, emotional, and uplifting \u2014 mix of pop and urban Latin sound. Should feel like a summer hit about admiration and genius.</p>"},{"location":"resultados/text_to_audio/#comentarios_2","title":"Coment\u00e1rios","text":"<p>Este \u00e1udio apresenta um estilo rom\u00e2ntico e celebrat\u00f3rio, com uma mistura de pop e reggaeton.</p>"},{"location":"resultados/text_to_audio/#comparacao-audios","title":"Compara\u00e7\u00e3o \u00e1udios","text":"Output Estilo/Guia Tempo (BPM) Observa\u00e7\u00e3o central 1 Pagodinho ac\u00fastico 100\u2013110 Clima ao vivo e backing harmonies refor\u00e7am o coro. 2 Rock/Nu-metal (PT) 90\u2013100 Din\u00e2mica verso/chorus com guitarras distorcidas. 3 Rock/Nu-metal (EN) 90\u2013100 Mesma est\u00e9tica, letra em ingl\u00eas, cl\u00edmax no refr\u00e3o. 4 \u00d3pera sinf\u00f4nica em 3 atos 80\u2192110 Orquestra/vozes, curva dram\u00e1tica em atos. 5 Latin pop/Reggaeton 90\u201395 Groove dan\u00e7ante, ponte em rap, refr\u00e3o marcante."},{"location":"resultados/text_to_audio/#comentarios_3","title":"Coment\u00e1rios","text":"<p>Esta tabela apresenta uma compara\u00e7\u00e3o dos diferentes estilos e caracter\u00edsticas dos \u00e1udios gerados.</p>"},{"location":"resultados/text_to_image/","title":"Resultados - Texto para Imagem","text":"<p>Nessa sec\u00e7\u00e3o ser\u00e3o apresentadas as imagens geradas com o worflow apresentado anteiormente (Implementa\u00e7\u00e3o). Al\u00e9m da apresenta\u00e7\u00e3o dos resultados encontrados, ser\u00e3o discutidos os par\u00e2metros utilizados em cada uma das gera\u00e7\u00f5es, bem como, o que era esperado de impacto com a mudan\u00e7a de cada um deles, e tamb\u00e9m, o que foi encontrado de fato ao mud=a-los. O objetivo principal, e utilizado como par\u00e2metro para an\u00e1lise das imagens geradas, era de criar uma imagem de um penguin surfando uma onda, tudo isso com um estilo de cartoon/anima\u00e7\u00e3o.</p>"},{"location":"resultados/text_to_image/#1o-output-baseline-do-workflow","title":"1\u00ba OUTPUT \u2014 (baseline do workflow)","text":""},{"location":"resultados/text_to_image/#parametros-utilizados","title":"Par\u00e2metros utilizados","text":"<ul> <li>Checkpoint: <code>SD15_cartoon.safetensors</code> (CLIP + MODEL + VAE do checkpoint)</li> <li>Prompt positivo (CLIP): \u201ca cute penguin surfing with a board on a big ocean wave, tropical beach background, clear blue sky, cinematic lighting, high quality, 4k render, dynamic motion, water splashes\u201d</li> <li>Prompt negativo (CLIP): \u201cblurry, deformed, low quality, missing wings, extra limbs, ugly, distorted, low resolution, bad anatomy, bad proportions, text, watermark, out of frame, grainy, over/underexposed, cartoonish\u201d</li> <li>Empty Latent Image: 512\u00d7512, batch_size 1</li> <li> <p>KSampler:</p> </li> <li> <p>seed: 859584950682642 (fixo na execu\u00e7\u00e3o)</p> </li> <li>steps: 40</li> <li>cfg: 20.0</li> <li>sampler: LMS</li> <li>scheduler: kl_optimal</li> <li>denoise: 1.00</li> <li>VAE Decode: VAE do pr\u00f3prio checkpoint</li> </ul>"},{"location":"resultados/text_to_image/#imagem-gerada","title":"Imagem gerada","text":"<p>Figura \u2014 baseline (meio do dia, 2 pinguins no longboard).</p>"},{"location":"resultados/text_to_image/#comparacao-output-x-objetivo-principal","title":"Compara\u00e7\u00e3o Output x Objetivo principal","text":"<p>A imagem correspondeu bem ao objetivo de ter um pinguim (na pr\u00e1tica, dois) surfando em \u00e1gua azul com c\u00e9u limpo, refor\u00e7ando o efeito de um CFG muito alto e steps suficientes para estabilizar contornos e texturas. A espuma e as bordas do longboard sa\u00edram n\u00edtidas, com brilho \u201cplastificado\u201d t\u00edpico do checkpoint cartoon. N\u00e3o h\u00e1 artefatos relevantes nem \u201cbanding\u201d no c\u00e9u, o que indica que o scheduler escolhido ajudou a suavizar gradientes. Um detalhe interessante \u00e9 que, mesmo com \u201ccartoonish\u201d no prompt negativo, o estilo permaneceu cartoon \u2014 isso evidencia que o vi\u00e9s do checkpoint predominou sobre a tentativa de \u201cdescartoonizar\u201d via negativo. Em resumo: alta fidelidade ao prompt, est\u00e9tica limpa e previs\u00edvel, com baixa variabilidade criativa.</p>"},{"location":"resultados/text_to_image/#2o-output-steps-baixos-cfg-baixo-enfoque-em-naturalidade","title":"2\u00ba OUTPUT \u2014 (steps baixos + cfg baixo; enfoque em naturalidade)","text":""},{"location":"resultados/text_to_image/#parametros-utilizados_1","title":"Par\u00e2metros utilizados","text":"<ul> <li>Prompt positivo (CLIP): \u201ca cute cartoon penguin surfing a breaking wave, medium shot, tropical beach in background, clear blue sky, gentle cinematic lighting, soft colors, simple composition, 4k, clean outlines, smooth shading\u201d</li> <li>Prompt negativo (CLIP): \u201cblurry, noisy, low quality, low resolution, bad anatomy, extra limbs, extra beak, text, watermark, logo, out of frame, over/underexposed, jpeg artifacts\u201d</li> <li> <p>KSampler:</p> </li> <li> <p>seed: 1337</p> </li> <li>steps: 15</li> <li>cfg: 6.5</li> <li>sampler: euler_a</li> <li>scheduler: karras</li> <li>denoise: 1.00</li> </ul>"},{"location":"resultados/text_to_image/#imagem-gerada_1","title":"Imagem gerada","text":"<p>Figura \u2014 composi\u00e7\u00e3o simples, espuma suave, menos detalhe fino.</p>"},{"location":"resultados/text_to_image/#comparacao-output-x-objetivo-principal_1","title":"Compara\u00e7\u00e3o Output x Objetivo principal","text":"<p>Aqui, a redu\u00e7\u00e3o dos steps e o CFG mais baixo produziram exatamente o comportamento esperado: a cena ficou mais \u201cpintada\u201d, com gradientes suaves e menos microdetalhe na espuma e no contorno do sujeito. O sampler ancestral favoreceu um tra\u00e7o mais solto e uma leve sensa\u00e7\u00e3o de movimento, ao custo de bordas menos r\u00edgidas. A composi\u00e7\u00e3o ficou simples e leg\u00edvel, com o pinguim destacado sobre a onda sem polui\u00e7\u00e3o visual. Pequenas varia\u00e7\u00f5es de modelagem do personagem (formato do bico, propor\u00e7\u00f5es do corpo) s\u00e3o coerentes com o aumento de liberdade criativa dado pelo CFG mais baixo. \u00c9 um bom exemplo de como \u201cabrir a m\u00e3o\u201d do guia torna o estilo mais org\u00e2nico e menos literal.</p>"},{"location":"resultados/text_to_image/#3o-output-detalhe-alto-foco-em-nitidez-e-consistencia","title":"3\u00ba OUTPUT \u2014 (detalhe alto; foco em nitidez e consist\u00eancia)","text":""},{"location":"resultados/text_to_image/#parametros-utilizados_2","title":"Par\u00e2metros utilizados","text":"<ul> <li>Prompt positivo (CLIP): \u201ctwo cute cartoon penguins surfing the same longboard, dynamic splash, close-up and sharp details, tropical waters, bright sunlight, crisp specular highlights, high detail, studio-quality render, 4k\u201d</li> <li>Prompt negativo (CLIP): \u201cblurry, low quality, low resolution, bad anatomy, extra fingers, extra wings, deformed beak, text, watermark, signature, out of frame, grainy, banding\u201d</li> <li> <p>KSampler:</p> </li> <li> <p>seed: 20251110</p> </li> <li>steps: 50</li> <li>cfg: 9.0</li> <li>sampler: dpmpp_2m</li> <li>scheduler: karras</li> <li>denoise: 1.00</li> </ul>"},{"location":"resultados/text_to_image/#imagem-gerada_2","title":"Imagem gerada","text":"<p>Figura \u2014 dois personagens lado a lado (pinguim + humanoide), look \u201ctoy\u201d.</p>"},{"location":"resultados/text_to_image/#comparacao-output-x-objetivo-principal_2","title":"Compara\u00e7\u00e3o Output x Objetivo principal","text":"<p>A proposta era obter dois pinguins em plano mais fechado, com alta nitidez. De fato, o aumento de steps e o uso de um sampler voltado a contornos limpos elevaram o n\u00edvel de detalhe em \u00e1gua, spray e materiais; por\u00e9m, o conte\u00fado derivou para um humanoide ao lado do pinguim. Essa deriva \u00e9 um sintoma cl\u00e1ssico de vi\u00e9s do dataset somado a um negativo pouco espec\u00edfico: sem proibi\u00e7\u00f5es expl\u00edcitas a \u201chuman/person/humanoid\u201d, o modelo \u201cpreenche\u201d a cena com o tipo de personagem que conhece bem para o estilo. Em termos qualitativos, o resultado \u00e9 tecnicamente bom (texturas e bordas est\u00e3o superiores aos cen\u00e1rios de menos steps), mas semanticamente abaixo do esperado. Como corre\u00e7\u00e3o, seria recomend\u00e1vel refor\u00e7ar o sujeito logo no in\u00edcio do prompt (\u201ctwo penguins\u2026\u201d) e colocar bloqueios negativos de humanos.</p>"},{"location":"resultados/text_to_image/#4o-output-direcao-estilistica-cel-shadinghq","title":"4\u00ba OUTPUT \u2014 (dire\u00e7\u00e3o estil\u00edstica: cel-shading/HQ)","text":""},{"location":"resultados/text_to_image/#parametros-utilizados_3","title":"Par\u00e2metros utilizados","text":"<ul> <li>Prompt positivo (CLIP): \u201ccartoon penguin surfing a huge ocean wave, cel-shading, thick ink outlines, comic book halftone texture, vibrant flat colors, exaggerated motion lines, dynamic angle, 4k, stylized illustration\u201d</li> <li>Prompt negativo (CLIP): \u201cphotorealistic, realistic skin, fine pores, photographic bokeh, text, watermark, logo, noisy, low quality, out of frame, deformed anatomy\u201d</li> <li> <p>KSampler:</p> </li> <li> <p>seed: 8675309</p> </li> <li>steps: 30</li> <li>cfg: 7.0</li> <li>sampler: euler</li> <li>scheduler: normal</li> <li>denoise: 1.00</li> </ul>"},{"location":"resultados/text_to_image/#imagem-gerada_3","title":"Imagem gerada","text":"<p>Figura \u2014 estilo cel-shading forte, contornos espessos; sujeito humanoide na crista da onda.</p>"},{"location":"resultados/text_to_image/#comparacao-output-x-objetivo-principal_3","title":"Compara\u00e7\u00e3o Output x Objetivo principal","text":"<p>A dire\u00e7\u00e3o de estilo para cel-shading foi atingida com clareza: cores chapadas, contornos grossos e sensa\u00e7\u00e3o de ilustra\u00e7\u00e3o de HQ. Entretanto, o conte\u00fado novamente cedeu para um surfista humano em vez do pinguim. O conjunto CFG moderado + scheduler normal favoreceu a domin\u00e2ncia do estilo sobre o tema, e, sem negativas expl\u00edcitas a humanos, o checkpoint cartoon \u201cpuxou\u201d para sua zona de conforto (personagem humano estilizado). O resultado \u00e9 \u00f3timo para demonstrar como termos de estilo fortes podem \u201cabafar\u201d o sujeito se o guia n\u00e3o for suficientemente enf\u00e1tico. Se o objetivo fosse manter o pinguim, bastaria subir levemente o CFG, mover os tokens de estilo para o final do prompt e incluir negativas de humano.</p>"},{"location":"resultados/text_to_image/#5o-output-composicao-ampla-por-do-sol-guia-forte-denoise-parcial","title":"5\u00ba OUTPUT \u2014 (composi\u00e7\u00e3o ampla, p\u00f4r do sol; guia forte + denoise parcial)","text":""},{"location":"resultados/text_to_image/#parametros-utilizados_4","title":"Par\u00e2metros utilizados","text":"<ul> <li>Prompt positivo (CLIP): \u201ccartoon penguin surfing at sunset, wide shot, golden hour lighting, dramatic sky with warm clouds, long lens compression, reflective water, motion blur on spray, cinematic composition, 4k\u201d</li> <li>Prompt negativo (CLIP): \u201cblurry subject, low quality, low resolution, bad anatomy, extra limbs, multiple heads, text, watermark, logo, out of frame, over/underexposed, color banding\u201d</li> <li> <p>KSampler:</p> </li> <li> <p>seed: 314159265</p> </li> <li>steps: 45</li> <li>cfg: 12.0</li> <li>sampler: dpmpp_sde</li> <li>scheduler: karras</li> <li>denoise: 0.85</li> </ul>"},{"location":"resultados/text_to_image/#imagem-gerada_4","title":"Imagem gerada","text":"<p>Figura \u2014 p\u00f4r do sol, paleta quente, pinguim de roupa de neoprene; gradientes suaves.</p>"},{"location":"resultados/text_to_image/#comparacao-output-x-objetivo-principal_4","title":"Compara\u00e7\u00e3o Output x Objetivo principal","text":"<p>Com p\u00f4r do sol, CFG alto, sampler SDE e denoise &lt; 1, o comportamento foi exemplar: paleta quente coesa, gradientes suaves no c\u00e9u e na \u00e1gua e um look mais \u201csoft\u201d que combina com a golden hour. A roupa de neoprene e o desenho mais \u201ccinematogr\u00e1fico\u201d do personagem mostram como o modelo respondeu ao pedido de composi\u00e7\u00e3o ampla e dram\u00e1tica, priorizando leitura e ilumina\u00e7\u00e3o. Perde-se um pouco de microcontraste em rela\u00e7\u00e3o ao baseline \u2014 efeito natural do denoise 0.85 \u2014, mas ganha-se coes\u00e3o de luz e atmosfera. \u00c9 o caso que melhor ilustra o alinhamento entre inten\u00e7\u00e3o de cena, par\u00e2metros e resultado visual.</p>"},{"location":"resultados/text_to_image/#comparacao-imagens","title":"Compara\u00e7\u00e3o imagens","text":"Output Prompt (resumo) Steps CFG Sampler Scheduler Denoise Observa\u00e7\u00e3o central 1 (baseline) Pinguim(s) surfando, c\u00e9u azul 40 20.0 LMS kl_optimal 1.00 Fidelidade alta, look limpo/cartoon 2 Pinguim, composi\u00e7\u00e3o simples/soft 15 6.5 euler_a karras 1.00 Menos detalhe, tra\u00e7o solto, suavidade 3 Dois pinguins, close, alto detalhe 50 9.0 dpmpp_2m karras 1.00 Nitidez alta; por\u00e9m derivou p/ humanoide 4 Cel-shading/HQ, linhas grossas 30 7.0 euler normal 1.00 Estilo forte; conte\u00fado derivou p/ humanoide 5 P\u00f4r do sol, wide, cinem\u00e1tico 45 12.0 dpmpp_sde karras 0.85 Gradientes suaves, look \u201csoft\u201d e fiel ao tema"},{"location":"resultados/text_to_image/#o-que-era-esperado-com-as-mudancas-dos-parametros","title":"O que era esperado com as mudan\u00e7as dos par\u00e2metros?","text":"<ul> <li>Steps: mais passos \u21d2 mais oportunidade de refino fino (bordas/espuma/mini-texturas). Menos passos \u21d2 aspecto pintado/sint\u00e9tico, maior variabilidade entre seeds.</li> <li>CFG: baixo \u21d2 naturalidade e liberdade estil\u00edstica (risco de se afastar do texto); alto \u21d2 fidelidade ao prompt (risco de satura\u00e7\u00e3o/artefatos ou \u201cengessamento\u201d).</li> <li> <p>Sampler:</p> </li> <li> <p>euler_a (ancestral): tra\u00e7o mais solto/vari\u00e1vel;</p> </li> <li>euler: tra\u00e7o firme, menos \u201cru\u00eddo criativo\u201d;</li> <li>dpmpp_2m: limpeza/nitidez de contornos;</li> <li>dpmpp_sde: gradientes e transi\u00e7\u00f5es suaves, bom para cenas com c\u00e9u/\u00e1gua.</li> <li>Scheduler: karras suaviza a progress\u00e3o do ru\u00eddo (menos \u201cquebra\u201d de tons); normal tende a resultados mais \u201ccrus/contrastados\u201d, o que em conjunto com CFG m\u00e9dio pode favorecer o estilo sobre o conte\u00fado.</li> <li>Denoise: 1.0 executa a cadeia completa (T2I puro); &lt;1.0 amortece o processo (\u00fatil para looks mais suaves ou para preservar estrutura em i2i).</li> </ul>"},{"location":"resultados/text_to_image/#o-que-foi-observado-com-as-mudancas","title":"O que foi observado com as mudan\u00e7as?","text":"<ul> <li>Output 1 confirmou que CFG muito alto (20) trava o conte\u00fado no prompt, com est\u00e9tica cartoon limpa.</li> <li>Output 2 mostrou o efeito combinado de steps baixos + euler_a + CFG baixo: suavidade, menos microdetalhe e mais \u201cpincelada\u201d.</li> <li>Output 3 confirmou a nitidez de dpmpp_2m com steps altos, mas revelou vi\u00e9s de dataset: sem negativo expl\u00edcito de \u201chuman\u201d, o modelo derivou para humanoide.</li> <li>Output 4 evidenciou que termos de estilo (cel-shading, comic) com CFG m\u00e9dio e scheduler normal podem sobrepor o conte\u00fado (humano em vez de pinguim).</li> <li>Output 5 entregou exatamente o esperado para p\u00f4r do sol: paleta quente, gradientes suaves e leve softness por conta do denoise 0.85, mantendo boa fidelidade (CFG 12).</li> </ul>"}]}